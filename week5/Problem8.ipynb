{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inga Ulusoy, Computational modelling in python, SoSe2020 \n",
    "\n",
    "Given are the following functions:\n",
    "\n",
    "\\begin{align}\n",
    "    f_1(x) &= x\\left(x-3\\right)\\left(x+3\\right) \\\\\n",
    "    f_2(x) &= \\left| x \\right| \\\\\n",
    "    f_3(x) &= \\sin \\left(2.1x\\right)\\left(-\\frac{x}{2}\\right) \\\\\n",
    "    f_4(x) &= 1.6^x -1.5x \\\\\n",
    "    f_5(x,y) &= \\sin\\left(x+y\\right)\\tan\\left(0.1x\\right) \\\\\n",
    "    f_6(x,y) &= \\sin\\left(\\sqrt{5}+x\\right)y \n",
    "\\end{align}\n",
    "\n",
    "\\- courtesy of Anna Bardroff \\- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def function1(x):\n",
    "    y = x*(x - 3)*(x + 3)\n",
    "    return y\n",
    "\n",
    "def function2(x):\n",
    "    y = abs(x)\n",
    "    return y\n",
    "\n",
    "def function3(x):\n",
    "    y = sin(x * 2.1) * (-x / 2.0)\n",
    "    return y\n",
    "\n",
    "def function4(x):\n",
    "    y = 1.6**x - 1.5 * x\n",
    "    return y\n",
    "\n",
    "def function5(x,y):\n",
    "    z = sin(x + y) * tan(0.1 * x)\n",
    "    return z\n",
    "\n",
    "def function6(x,y):\n",
    "    z = sin(sqrt(5) + x) * y\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to differentiate a function\n",
    "\n",
    "## Numerically from the definition of the derivative\n",
    "\n",
    "$f'(x) \\approx \\frac{f(x+h)-f(x)}{h}$\n",
    "\n",
    "with $h \\rightarrow 0$. \n",
    "\n",
    "- using the numpy diff function:\n",
    "https://numpy.org/doc/1.18/reference/generated/numpy.diff.html \\\n",
    "This function returns the difference between two consecutive elements of an array, as $f(x_{i+1})-f(x_i)$. Division by $dx$ will result in the above derivative. As diff() computes the differences, the returned array will have one fewer element than the original array for the first derivative. With diff also higher derivatives can be constructed (through recursive application of the difference) as shown below. \n",
    "\n",
    "- using the numpy gradient function:\n",
    "https://numpy.org/doc/1.18/reference/generated/numpy.gradient.html#numpy.gradient \\\n",
    "This function requires dx as an input and will compute the gradient using central differences for the interior points and one-side differences at the boundaries and will thus return an array of the same shape as the original array. Higher derivatives must be constructed by applying gradient repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=-4\n",
    "b=4\n",
    "npoints = 20\n",
    "num,dx = linspace(a,b,npoints,retstep=True)\n",
    "f1 = function1(num)\n",
    "dydx_diff = diff(f1)/dx\n",
    "dydx_grad = gradient(f1, dx,edge_order=2)\n",
    "dydx_gradn = gradient(f1, dx,edge_order=1)\n",
    "\n",
    "num2 = num + dx/2\n",
    "\n",
    "mf=18\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(num,f1,label='f1')\n",
    "plt.plot(num2[0:-1],dydx_diff,label='diff')\n",
    "plt.plot(num,dydx_grad,label='gradient,order=2')\n",
    "plt.plot(num,dydx_gradn,label='gradient,order=1')\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "legend = ax.legend(loc='lower right', shadow=False,fontsize=mf,borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second derivative\n",
    "d2ydx2_diff = diff(f1,n=2)/dx**2\n",
    "d2ydx2_grad = gradient(dydx_grad, dx, edge_order=2)\n",
    "num3 = num2 + dx/2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(num,f1,label='f1')\n",
    "plt.plot(num2[0:-1],dydx_diff,label='diff')\n",
    "plt.plot(num,dydx_grad,label='gradient')\n",
    "plt.plot(num3[0:-2],d2ydx2_diff,label='diff2')\n",
    "plt.plot(num,d2ydx2_grad,label='gradient2')\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "legend = ax.legend(loc='lower right', shadow=False,fontsize=mf,borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it is obvious which one of the two routines is cleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using autograd\n",
    "\n",
    "https://pypi.org/project/autograd/ \\\n",
    "https://github.com/HIPS/autograd \\\n",
    "Open the anaconda terminal and type\\\n",
    "`pip install autograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.numpy import *\n",
    "from autograd import grad\n",
    "dydx_autograd=[]\n",
    "grad_fct = grad(function1)\n",
    "for i in num:\n",
    "    dydx_autograd.append(grad_fct(i))\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(num,f1,label='f1')\n",
    "plt.plot(num,dydx_autograd,label='autograd')\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "legend = ax.legend(loc='lower right', shadow=False,fontsize=mf,borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or directly using arrays as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import elementwise_grad as egrad \n",
    "\n",
    "dydx_autograd=egrad(function1)(num)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(num,f1,label='f1')\n",
    "plt.plot(num,dydx_autograd,label='autograd')\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "legend = ax.legend(loc='lower right', shadow=False,fontsize=mf,borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second derivative\n",
    "dydx_autograd=egrad(function1)(num)\n",
    "d2ydx2_autograd=egrad(egrad(function1))(num)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(num,f1,label='f1')\n",
    "plt.plot(num,dydx_autograd,label='autograd')\n",
    "plt.plot(num,d2ydx2_autograd,label='autograd2')\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "legend = ax.legend(loc='lower right', shadow=False,fontsize=mf,borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously a method of choice, especially for applications involving neural networks where many derivatives are required (and is used in PyTorch, see for example - https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolically using sympy\n",
    "\n",
    "Sympy is a python library for symbolic manipulations: https://www.sympy.org/en/index.html. \n",
    "\n",
    "I want to mention it here for completeness, but generally we will not perform any symbolic computations. Personally I prefer mathematica for that, but if you need symbolic manipulations and do not know mathematica yet, maybe sympy is the easier way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x= sp.Symbol('x')\n",
    "\n",
    "function= x*(x-3)*(x+3)\n",
    "dydx_sp=sp.diff(function, x)\n",
    "print(dydx_sp)\n",
    "\n",
    "#it comes with its own plotting routine\n",
    "p1 = sp.plot(function,(x,-4,4),label='f1',show=False)\n",
    "p2 = sp.plot(dydx_sp,(x,-4,4),label='sympy',show=False)\n",
    "p1.append(p2[0])\n",
    "p1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial derivatives\n",
    "\n",
    "The derivative of a function $f(x_1,x_2,\\ldots,x_n)$ of more than one variable with respect to one of those variables is given by the partial derivative of the function, $\\frac{\\partial f}{\\partial x_1} (x_1,x_2,\\ldots,x_n)$.\n",
    "\n",
    "The Jacobian determinant contains the first partial derivatives of a vectorized function $\\mathbf{f} (x_1,x_2,\\ldots,x_n)$ wrt all the dimensions:\n",
    "\\begin{align}\n",
    "\\mathbf{J} = \\left[ \\frac{\\partial \\mathbf{f}}{\\partial x_1}, \\frac{\\partial \\mathbf{f}}{\\partial x_2}, \\ldots, \\frac{\\partial \\mathbf{f}}{\\partial x_n} \\right] \n",
    "\\end{align}\n",
    "\n",
    "For example: \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{f} \\left(\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \n",
    "\\end{bmatrix}\n",
    "\\right)=\n",
    "\\begin{bmatrix} f_1(x,y) \\\\\n",
    "f_2(x,y) \\end{bmatrix} = \n",
    "\\begin{bmatrix} x^2+y  \\\\\n",
    "y+2y^2+x \\end{bmatrix} \n",
    "\\end{align}\n",
    "The Jacobian matrix of this function is\n",
    "\\begin{align}\n",
    "\\mathbf{J}(x,y) =\n",
    "\\begin{bmatrix} \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} \\end{bmatrix} = \n",
    "\\begin{bmatrix} 2x & 1  \\\\\n",
    "1 & 4y + 1 \\end{bmatrix} \n",
    "\\end{align}\n",
    "and the Jacobian determinant is \n",
    "\\begin{align}\n",
    "\\det(\\mathbf{J}(x,y))  =\n",
    "(2x)\\cdot (4y+1) - 1 \\cdot 1 = 8xy + 2x - 2\n",
    "\\end{align}\n",
    "The Jacobian matrix maps a set of vectors onto another set of vectors, for example, the transformation from cartesian $x, y$ to polar $r, \\theta$ coordinates can be described through a Jacobian. __Because of this mapping, the Jacobian matrix is a tensor__. More precisely, it is a rank-2 tensor.\\\n",
    "https://mathworld.wolfram.com/TensorRank.html\n",
    "\n",
    "The Jacobian determinant is useful because it tells how the volume changes under the map.\n",
    "\n",
    "__This is an example of matrix calculus, which is fundamental to applications in quantum chemistry and machine learning__.\n",
    "\n",
    "Let's look at partial derivatives for an example function. We will use `autograd` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our sample function\n",
    "def myfunc(x,y):\n",
    "    z = x**2+y\n",
    "    return z\n",
    "\n",
    "#let's define a grid for evaluation and plotting so we can check the result\n",
    "a=-4\n",
    "b=4\n",
    "npoints = 100\n",
    "numx,dx = linspace(a,b,npoints,retstep=True)\n",
    "numy,dy = linspace(a,b,npoints,retstep=True)\n",
    "#this is the mesh for the plotting\n",
    "pdx = zeros((len(numx),len(numy)))\n",
    "pdy = zeros((len(numx),len(numy)))\n",
    "fin = zeros((len(numx),len(numy)))\n",
    "#derivative wrt first variable - x\n",
    "dx = grad(myfunc,0)\n",
    "#derivative wrt second variable - y\n",
    "dy = grad(myfunc,1)\n",
    "for j,x in enumerate(numx):\n",
    "    for k,y in enumerate(numy):\n",
    "        #value of the function at x,y\n",
    "        fin[j,k]=myfunc(x,y)\n",
    "        #value of partial derivative wrt x at x,y\n",
    "        pdx[j,k]=dx(x,y)\n",
    "        #value of partial derivative wrt y at x,y\n",
    "        pdy[j,k]=dy(x,y)\n",
    "            \n",
    "def plot2D(x,y,z,title):\n",
    "    mf = 16\n",
    "    Y, X = meshgrid(y,x)\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X, Y, z,cmap = cm.viridis,rstride=1, cstride=1,edgecolor='none')\n",
    "    plt.xticks(fontsize=mf)\n",
    "    plt.yticks(fontsize=mf)\n",
    "    ax.zaxis.set_tick_params(labelsize=mf)\n",
    "    ax.set_xlabel('x value',fontsize=mf)\n",
    "    ax.set_ylabel('y value',fontsize=mf)\n",
    "    ax.set_zlabel('z value',fontsize=mf)\n",
    "    ax.set_title('{}'.format(title),fontsize=mf, weight='bold')\n",
    "    ax.view_init(30, 80)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_im(x,y,z,title):\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.imshow(z,extent=[x[0],x[-1],y[0],y[-1]])\n",
    "    ax.set_title('{}'.format(title),fontsize=mf, weight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2D(numx,numy,fin,'myfunc')\n",
    "plot2D(numx,numy,pdx,'df/dx')\n",
    "plot2D(numx,numy,pdy,'df/dy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_im(numx,numy,fin,'myfunc')\n",
    "plot_im(numx,numy,pdx,'df/dx')\n",
    "plot_im(numx,numy,pdy,'df/dy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second derivatives: The Laplacian\n",
    "\n",
    "The Laplace operator generates the second derivatives of a multidimensional function, for example, in Hilbert space:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta f(x_1,x_2,\\ldots,x_n) = \n",
    "\\triangledown ^2  f(x_1,x_2,\\ldots,x_n) \n",
    "= \\sum_i^n \\frac{\\partial^2 f}{\\partial x_i^2}\n",
    "\\end{align}\n",
    "The Laplacian is extremely important in physics and appears for example in the Schr√∂dinger equation. For a numerical representation of the Laplacian, the Laplacian matrix is introduced:\n",
    "\\begin{align}\n",
    "\\mathcal{L} = \\mathbf{D}-\\mathbf{A} \n",
    "\\end{align}\n",
    "where $\\mathbf{D}$ is the degree matrix (a diagonal matrix containing the degree of each vertex $v_i$) and $\\mathbf{A}$ is the adjacency matrix (which is 0 everywhere and -1 for adjacent vertices $v_i$ and $v_j$. As such, the Laplacian matrix contains the degree on the diagonal and ones on the off-diagonal for adjacent vertices.\n",
    "\n",
    "Let's look at a one-dimensional example, using three-point finite differences for the evaluation of the second derivative\n",
    "\\begin{align}\n",
    "\\frac{d^2 f}{dx^2} = \\frac{f(x_{j-1})-2f(x_j)+f(x_{j+1})}{h^2}\n",
    "\\end{align}\n",
    "The second derivative operator will appear on the diagonal on the Laplacian matrix and one on the off-diagonal for adjacent grid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=-4\n",
    "b=4\n",
    "nsteps=9\n",
    "myvals,h=linspace(a,b,nsteps,retstep=True)\n",
    "Laplacian=(-2.0*diag(ones(nsteps))+diag(ones(nsteps-1),1)+diag(ones(nsteps-1),-1))/(float)(h**2)\n",
    "print('Laplacian:\\n',Laplacian)\n",
    "print('Diagonal matrix:\\n',diag(ones(nsteps)))\n",
    "print('-Adjacency matrix part 1:\\n',diag(ones(nsteps-1),1))\n",
    "print('-Adjacency matrix part 2:\\n',diag(ones(nsteps-1),-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to a sample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc2(x):\n",
    "    y = 2*x**3\n",
    "    return y\n",
    "\n",
    "a=-1\n",
    "b=1\n",
    "nsteps=100\n",
    "myvals,h=linspace(a,b,nsteps,retstep=True)\n",
    "Laplacian=(-2.0*diag(ones(nsteps))+diag(ones(nsteps-1),1)+diag(ones(nsteps-1),-1))/(float)(h**2)\n",
    "inp = myfunc2(myvals)\n",
    "outp = dot(Laplacian,inp)\n",
    "print(outp)\n",
    "#delete the endpoints, because we would need a special formula for them and do not want to bother at this point\n",
    "outp = delete(outp,0)\n",
    "outp = delete(outp,-1)\n",
    "print(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(myvals,inp,label='f(x)')\n",
    "plt.plot(myvals[1:-1],outp,marker='x',markevery=10,label=r'd$^2$ f(dx$^2$), L')\n",
    "plt.plot(myvals,12*myvals,linestyle = ':',label=r'd$^2$ f(dx$^2$),exact')\n",
    "legend = plt.legend(loc='upper left', shadow=False,fontsize=16,borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Compute and plot the first and second derivative of the above one-dimensional functions using your method of choice in the value range from -4 to 4 using an appropriate grid spacing.\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Compute and plot the partial first derivatives of the above two-dimensional functions using autograd in the value range from -4 to 4 using an appropriate grid spacing. \n",
    "\n",
    "# Task 3\n",
    "Construct and plot the second derivative of function1, $d^2f_1/dx^2$, through the Laplacian matrix in the value range from -4 to 4 using an appropriate grid spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
