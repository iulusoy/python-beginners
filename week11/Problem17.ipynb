{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inga Ulusoy, Computational modelling in python, SoSe2020 \n",
    "\n",
    "# Machine learning using neural networks and Keras/TensorFlow\n",
    "\n",
    "Parts of the notebooks are taken from \n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/ml-intro\n",
    "\n",
    "\n",
    "## Neural networks\n",
    "\n",
    "If you would like to implement your own neural network, you may take a look at this page:\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "\n",
    "However, we will use the framework provided by Keras/TensorFlow in the following and not start from scratch.\n",
    "\n",
    "### Feedforward\n",
    "The output is constructed as\n",
    "\\begin{align}\n",
    " z_1 & = W^{(1)}x + b^{(1)} \\\\\n",
    " z_2 & = W^{(2)} \\sigma(z_1) + b^{(2)} \\\\\n",
    " y &= \\sigma(z_2)\n",
    " \\end{align}\n",
    " The \"correct\" weights $W$ and biases $b$ are obtained through training the neural network.\n",
    " The function $\\sigma$ is the activation function. Many different activation functions are in use, with the sigmoid function being a popular function:\n",
    " \\begin{align}\n",
    " \\sigma(z)=\\frac{1}{1+\\exp(-z)}\n",
    " \\end{align}\n",
    "### Loss function\n",
    "We will use a simple sum-of-squares (least squares) loss function (see Problem2):\n",
    "\\begin{align}\n",
    "R^2=\\sum_{i=1}^n [y_{i,\\rm{exact}}-y_{i}]^2\n",
    " \\end{align}\n",
    "However, any other loss function also works, and it depends on the problem that is considered. The value $y_{i,\\rm{exact}}$ is the exact solution (the actual $y$) from the dataset that is used in the training.\n",
    "\n",
    "### Backpropagation\n",
    "We need to follow the loss function towards its minimum. As $R^2$ does not directly depend on $W$ and $b$, we apply the chain rule:\n",
    "\\begin{align}\n",
    "\\frac{\\partial R^2}{\\partial W^{(2)}} = \\frac{\\partial R^2}{\\partial y} \\frac{\\partial y}{\\partial z_2} \\frac{\\partial z_2}{\\partial W^{(2)}} \n",
    "\\end{align}\n",
    "and similarly for $b^{(2)}$. In this specific example, for the above equation we obtain\n",
    "\\begin{align}\n",
    "\\frac{\\partial R^2}{\\partial W^{(2)}} =2 (y_{i,\\rm{exact}}-y_{i})\n",
    "\\cdot \\frac{\\partial \\sigma}{\\partial z_2} \n",
    "\\cdot \\left( \\sigma(z_1) + b^{(2)} \\right)\n",
    "\\end{align}\n",
    "and\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\sigma}{\\partial z_2}= z_2(1-z_2)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "wDlWLbfkJtvu"
   },
   "outputs": [],
   "source": [
    "#Modified google colab\n",
    "\n",
    "#@title Copyright 2020 Google LLC. Double-click here for license information.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to use version 2.x of TensorFlow. You may need to \n",
    "\n",
    "`pip install tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use numpy only once here so I import it as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to initialize the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(my_learning_rate):\n",
    "    \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "    # Most simple tf.keras models are sequential. \n",
    "    # A sequential model contains one or more layers.\n",
    "    # Each layer has one input and one output.\n",
    "    # A non-sequential model has branching in the layers (multiple inputs or outputs)\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Describe the topography of the model.\n",
    "    # The topography of a simple linear regression model\n",
    "    # is a single node in a single layer. \n",
    "    # Dense means that it is a densely connected NN\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "    # units specifies the number of layers (dimensionality) - here one\n",
    "    # input_shape specifies the dimensionality of the input - here one\n",
    "    model.add(tf.keras.layers.Dense(units=1, \n",
    "                                  input_shape=(1,)))\n",
    "\n",
    "    # Compile the model topography into code that \n",
    "    # TensorFlow can efficiently execute. \n",
    "    # Configure training to minimize the model's mean squared error. \n",
    "    # compile is a method of model, and will effectively configure the model \n",
    "    # for training.\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the training function of the model. You need to pass the hyperparameters such as epochs and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, feature, label, epochs, batch_size):\n",
    "    \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "    # Feed the feature values and the label values to the \n",
    "    # model. The model will train for the specified number \n",
    "    # of epochs, gradually learning how the feature values\n",
    "    # relate to the label values. \n",
    "    history = model.fit(x=feature,\n",
    "                        y=label,\n",
    "                        batch_size=None,\n",
    "                        epochs=epochs)\n",
    "  \n",
    "    # Gather the trained model's weight and bias.\n",
    "    trained_weight = model.get_weights()[0]\n",
    "    trained_bias = model.get_weights()[1]\n",
    "  \n",
    "    # The list of epochs is stored separately from the \n",
    "    # rest of history.\n",
    "    epochs = history.epoch\n",
    "    \n",
    "    # Gather the history (a snapshot) of each epoch.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "  \n",
    "    # Specifically gather the model's root mean \n",
    "    #squared error at each epoch. \n",
    "    rmse = hist[\"root_mean_squared_error\"]\n",
    "  \n",
    "    return trained_weight, trained_bias, epochs, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some plotting functions that help us visualize the output from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "    \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "  \n",
    "    # Label the axes.\n",
    "    plt.xlabel(\"feature\")\n",
    "    plt.ylabel(\"label\")\n",
    "  \n",
    "    # Plot the feature values vs. label values.\n",
    "    plt.scatter(feature, label)\n",
    "  \n",
    "    # Create a red line representing the model. The red line starts\n",
    "    # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "    x0 = 0\n",
    "    y0 = trained_bias\n",
    "    x1 = my_feature[-1]\n",
    "    y1 = trained_bias + (trained_weight * x1)\n",
    "    plt.plot([x0, x1], [y0, y1], c='r')\n",
    "  \n",
    "    # Render the scatter plot and the red line.\n",
    "    plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, rmse):\n",
    "    \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Root Mean Squared Error\")\n",
    "  \n",
    "    plt.plot(epochs, rmse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([rmse.min()*0.97, rmse.max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataset\n",
    "\n",
    "We will start with some simple artificial (\"synthetic\") data.\n",
    "\n",
    "We will use the volume vs extinction data from Problem2 (Problem16), slightly linearized for the high volumes and with a few additional points. We add some noise to the data to make it more realistic. \n",
    "\n",
    "This dataset consists of 11 examples; the features (input x) are stored in `my_feature` and the label (output y) is stored in `my_label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature = ([2.5,5.0,7.5,10.0,12.5,15.0,17.5,20.0,22.5,25.0,30.0])\n",
    "my_label   = ([0.097, 0.195, 0.289, 0.387, 0.483, 0.581, 0.686, 0.792,0.890, 0.982, 1.166])\n",
    "plt.plot(my_feature,my_label,marker='x')\n",
    "#now add some noise\n",
    "noise = np.random.normal(0,0.05,len(my_feature))\n",
    "my_label+=noise\n",
    "my_label = list(my_label)\n",
    "plt.scatter(my_feature,my_label,marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The hyperparameters\n",
    "\n",
    "The hyperparameters are: the learning rate, the epochs, the batch size. The learning rate is\n",
    "\\begin{align}\n",
    "\\mathrm{learning\\:rate} = \\frac{\\mathrm{gradient\\:magnitude}}{\\mathrm{step\\:size}}\n",
    "\\end{align}\n",
    "\n",
    "Build and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.005\n",
    "epochs=10\n",
    "my_batch_size=11\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                         my_label, epochs,\n",
    "                                                         my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss curve has not flattened out. The model is not trained completely. The dots in the first plot specify the correct y and the red line the predicted line. If you execute the above cell several times, the result will look different as randomness plays a role in the evaluation.\n",
    "\n",
    "### Increase the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.005\n",
    "epochs= 200   \n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                        my_label, epochs,\n",
    "                                                        my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the initial random numbers of the weights, you will observe that the loss curve may flatten out, but not completely, and the fit may still not be very good. In addition, this took quite a while to run due to the high number of epochs.\n",
    "\n",
    "### Increase the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the learning rate and decrease the number of epochs.\n",
    "learning_rate=1\n",
    "epochs=50\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                         my_label, epochs,\n",
    "                                                         my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see lots of bumps and peaks in the loss curve. The learning rate is too high.\n",
    "\n",
    "### Find the ideal combination of learning rate and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate=0.022\n",
    "learning_rate=0.010\n",
    "epochs=250\n",
    "my_batch_size=11\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                         my_label, epochs,\n",
    "                                                         my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This somewhat depends on the random noise of the initial dataset. It is quite difficult to adjust the parameters!\n",
    "\n",
    "\n",
    "### Adjust the batch size\n",
    "\n",
    "The system recalculates the model's loss value and adjusts the model's weights and bias after each **iteration**.  Each iteration is the span in which the system processes one batch. For example, if the **batch size** is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples.  \n",
    "\n",
    "One **epoch** spans sufficient iterations to process every example in the dataset. For example, if the batch size is 11, then each epoch lasts one iteration. However, if the batch size is 6, then each epoch consumes two iterations.  \n",
    "\n",
    "It is tempting to simply set the batch size to the number of examples in the dataset (11, in this case). However, the model might actually train faster on smaller batches. Conversely, very small batches might not contain enough information to help the model converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.02\n",
    "epochs=250\n",
    "my_batch_size=6\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                        my_label, epochs,\n",
    "                                                        my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of hyperparameter tuning\n",
    "\n",
    "Most machine learning problems require a lot of hyperparameter tuning.  Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly.  You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb:\n",
    "\n",
    " * Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. \n",
    " * If the training loss does not converge, train for more epochs.\n",
    " * If the training loss decreases too slowly, increase the learning rate. Note that setting the training loss too high may also prevent training loss from converging.\n",
    " * If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.\n",
    " * Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\n",
    " * Setting the batch size to a *very* small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\n",
    " * For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory. \n",
    "\n",
    "Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3014ezH3C7jT"
   },
   "source": [
    "# A neural net model\n",
    "\n",
    "The `create_model` function defines the topography of the deep neural net, specifying the following:\n",
    "\n",
    "* The number of [layers](https://developers.google.com/machine-learning/glossary/#layer) in the deep neural net.\n",
    "* The number of [nodes](https://developers.google.com/machine-learning/glossary/#node) in each layer.\n",
    "\n",
    "The `create_model` function also defines the [activation function](https://developers.google.com/machine-learning/glossary/#activation_function) of each layer. Here the rectifier is used\n",
    "\\begin{align}\n",
    "f(x)=x^{+}=\\max(0,x)\n",
    "\\end{align}\n",
    "with $x$ the node input, and it is very popular for deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check version\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use numpy only once here so I import it as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "my_feature = ([2.5,5.0,7.5,10.0,12.5,15.0,17.5,20.0,22.5,25.0,30.0])\n",
    "my_label   = ([0.097, 0.195, 0.289, 0.387, 0.483, 0.581, 0.686, 0.792,0.890, 0.982, 1.166])\n",
    "plt.plot(my_feature,my_label,marker='x')\n",
    "#now add some noise\n",
    "noise = np.random.normal(0,0.05,len(my_feature))\n",
    "my_label+=noise\n",
    "my_label = list(my_label)\n",
    "plt.scatter(my_feature,my_label,marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "def create_model(my_learning_rate):\n",
    "    \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the layer containing the feature columns to the model.\n",
    "    # this expects the input data to hold 11 values\n",
    "    # it adds one hidden layer with 10 nodes\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1, input_shape=(1,)))\n",
    "    \n",
    "\n",
    "    #   * units specifies the number of nodes in this layer.\n",
    "    #   * activation specifies the activation function (Rectified Linear Unit).\n",
    "    #   * name is just a string that can be useful when debugging.\n",
    "    # Define the hidden layer with 11 nodes.   \n",
    "    model.add(tf.keras.layers.Dense(units=11, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden1'\n",
    "                                   ))\n",
    "  \n",
    "    # Define the output layer.\n",
    "    model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                  name='Output'))                              \n",
    "  \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anH4A_yCcZx2"
   },
   "source": [
    "Define a training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jv_lJYTcrEF"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label_name, feature_name,\n",
    "                batch_size=None):\n",
    "    \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "    # Split the dataset into features and label. This generates a dictionary\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    #print(features)\n",
    "    \n",
    "    label = np.array(features.pop(label_name))\n",
    "    #print(label)\n",
    "    feature = np.array(features.pop(feature_name))\n",
    "    history = model.fit(x=feature, y=label, batch_size=batch_size,\n",
    "           epochs=epochs, shuffle=True) \n",
    "\n",
    "    # The list of epochs is stored separately from the rest of history.\n",
    "    epochs = history.epoch\n",
    "  \n",
    "    # To track the progression of training, gather a snapshot\n",
    "    # of the model's mean squared error at each epoch. \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    mse = hist[\"mean_squared_error\"]\n",
    "    \n",
    "    trained_weight = model.get_weights()[0]\n",
    "    trained_bias = model.get_weights()[1]\n",
    "\n",
    "    return trained_weight, trained_bias, epochs, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-IXYVfvM4gD"
   },
   "source": [
    "## Call the functions to build and train a deep neural net\n",
    "\n",
    "We need to rephrase the input as a pandas dataframe and hand it to tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(list(zip(my_feature, my_label)),columns=['volume','extinction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tensorflow knows how the features are structured. At this point, my_feature_layer is not assigned any values, but just the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "nj3v5EKQFY8s"
   },
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "batch_size = 4\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"extinction\"\n",
    "feature_name = \"volume\"\n",
    "\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate)\n",
    "\n",
    "# Train the model on the training set.\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, train_df, epochs,\n",
    "                                                        label_name,feature_name,\n",
    "                                                        batch_size)\n",
    "\n",
    "#plotting the weights and biases only works for the linear model with one layer\n",
    "#plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "#with more hidden layers, there is no simple linear relationship that allows the plotting\n",
    "#of a linear output function - it is more complex than that\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "nj3v5EKQFY8s"
   },
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "batch_size = 11\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"extinction\"\n",
    "feature_name = \"volume\"\n",
    "\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate)\n",
    "\n",
    "# Train the model on the training set.\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, train_df, epochs,\n",
    "                                                        label_name,feature_name,\n",
    "                                                        batch_size)\n",
    "\n",
    "#plotting the weights and biases only works for the linear model with one layer\n",
    "#plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "#with more hidden layers, there is no simple linear relationship that allows the plotting\n",
    "#of a linear output function - it is more complex than that\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's generate some test data to check the performance of the model\n",
    "from scipy import interpolate\n",
    "func = interpolate.interp1d(my_feature, my_label)\n",
    "test_x = np.linspace(my_feature[0],my_feature[-1],10)\n",
    "plt.plot(test_x,func(test_x))\n",
    "plt.scatter(my_feature, my_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select a few sample feature values and compute the predicted labels. For this purpose, we need to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = test_x\n",
    "test_label = func(test_x)\n",
    "print(\"Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlPXK-SmmjQ2"
   },
   "source": [
    "# Task 1 \n",
    "\n",
    "Modify the number of nodes in the layer of the neural network and the hyperparameters. \n",
    "\n",
    "- What is the final loss in the training?\n",
    "- What is the loss for the test set?\n",
    "\n",
    "Summarize your results and consider Occam's razor in discussing your results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Intro to Neural Nets.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
