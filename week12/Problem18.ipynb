{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inga Ulusoy, Computational modelling in python, SoSe2020 \n",
    "\n",
    "# A neural network using data from the periodic table: Regression\n",
    "\n",
    "We will now build a NN using data from the periodic table and make predictions about the atomic radius. In a regression problem, the output of the NN is a continuous value (a float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The example dataset\n",
    "\n",
    "We will train a model to predict the van der Waals radius of an atom, given the atomic mass. As training data, we will use a subset of the periodic table.\n",
    "\n",
    "The dataset is taken from here:\n",
    "\n",
    "https://www.kaggle.com/jwaitze/tablesoftheelements\n",
    "\n",
    "On kaggle, you may find many other datasets that you can train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the dataset\n",
    "#if the read fails for you, you may try different encoding\n",
    "#there will be some ? in the column with 'pronounciation'\n",
    "#\n",
    "#\n",
    "#test_df = pd.read_csv('periodic_table_with_all_units.csv',encoding = 'ISO-8859-1',engine='python')\n",
    "#test_df = pd.read_csv('periodic_table_with_all_units.csv',encoding = 'utf-8',engine='python')\n",
    "test_df = pd.read_csv('periodic_table_with_all_units.csv',encoding = 'cp1252',engine='python')\n",
    "pd.set_option('display.max_columns', None)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has way too much information. We will create a new dataframe with only the columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.concat([test_df['number'], test_df['name'], test_df['symbol'], test_df['element_category'], \n",
    "                    test_df['atomic_weight'], test_df['electron_configuration'], \n",
    "                    test_df['covalent_radius'],test_df['van_der_waals_radius']], axis=1)\n",
    "mydata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to clean up the data a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the parenthesis (xx) in atomic weight\n",
    "print(mydata.atomic_weight.head(10))\n",
    "old_data = mydata.atomic_weight.values\n",
    "new_data = mydata.atomic_weight.str.split('(').str[0]\n",
    "mydata.atomic_weight = mydata.atomic_weight.replace(old_data,new_data)\n",
    "print(mydata.atomic_weight.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove trailing spaces from the strings\n",
    "mydata.atomic_weight = mydata.atomic_weight.str.strip()\n",
    "#remove the [] in atomic_weight\n",
    "mydata['atomic_weight'] = mydata['atomic_weight'].str.replace('[', '')\n",
    "mydata['atomic_weight'] = mydata['atomic_weight'].str.replace(']', '')\n",
    "#convert to float\n",
    "mydata.atomic_weight = pd.to_numeric(mydata.atomic_weight)\n",
    "mydata.atomic_weight.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the parenthesis (xx) in covalent radius\n",
    "print(mydata.covalent_radius.head(10))\n",
    "old_data = mydata.covalent_radius.values\n",
    "new_data = mydata.covalent_radius.str.split('(').str[0]\n",
    "mydata.covalent_radius = mydata.covalent_radius.replace(old_data,new_data)\n",
    "print(mydata.covalent_radius.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the covalent radius we need only the first number\n",
    "#as the formatting is not consistent, pandas read this column as 'object' datatype\n",
    "#(basically the entries have different datatypes, some are lists, some are strings)\n",
    "#first we make all the entries strings\n",
    "old_data = mydata.covalent_radius.values\n",
    "new_data = mydata.covalent_radius.str.join('')\n",
    "#now we split them at the +-\n",
    "new_data = new_data.str.split('±').str[0]\n",
    "#we further remove the 'pm'\n",
    "new_data = new_data.str.split(' ').str[0]\n",
    "#we further remove the '-'\n",
    "new_data = new_data.str.split('–').str[0]\n",
    "mydata.covalent_radius = mydata.covalent_radius.replace(old_data,new_data)\n",
    "#convert to float and put nan for non-numeric values\n",
    "mydata.covalent_radius = pd.to_numeric(mydata.covalent_radius,errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the 'pm' in van_der_waals_radius\n",
    "print(mydata.van_der_waals_radius.head(5))\n",
    "old_data = mydata.van_der_waals_radius.values\n",
    "new_data = mydata.van_der_waals_radius.str.split('p').str[0]\n",
    "mydata.van_der_waals_radius = mydata.van_der_waals_radius.replace(old_data,new_data)\n",
    "print(mydata.van_der_waals_radius.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mydata.number.values)\n",
    "print(mydata.atomic_weight.values)\n",
    "print(mydata.covalent_radius.values)\n",
    "print(mydata.van_der_waals_radius.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with nan\n",
    "mydata = mydata.dropna()\n",
    "#reset the row index to number consecutively\n",
    "mydata = mydata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into a training and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mydata = mydata.sample(frac=0.8,random_state=20)\n",
    "test_mydata = mydata.drop(train_mydata.index)\n",
    "train_mydata.head(100)\n",
    "#test_mydata.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The descriptors\n",
    "\n",
    "In the following, we want to use this data to predict \n",
    "\n",
    "1. the covalent radius from the atomic weight, van der Waals radius and proton number;\n",
    "2. the covalent radius from the atomic weight, van der Waals radius, and number of protons (the atomic number).\n",
    "\n",
    "We thus have multiple features in our data that we will use to derive a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pairwise correlation matrix to determine linear connectivity of the values\n",
    "# a high value means high correlation\n",
    "train_mydata.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "corr_matrix = train_mydata.corr()\n",
    "sn.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sn.pairplot(train_mydata[[\"number\", \"atomic_weight\", \"covalent_radius\", \"van_der_waals_radius\"]])\n",
    "sn.pairplot(train_mydata[[\"number\", \"atomic_weight\", \"covalent_radius\", \"van_der_waals_radius\"]], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ldP-5z1B2vL"
   },
   "source": [
    "Covalent radius is the most correlated with van der Waals radius.\n",
    "\n",
    "## Normalize values\n",
    "\n",
    "When building a model with multiple features, the values of each feature should cover roughly the same range. Thus we need to normalize the data; here we take the Z-score or standard score:\n",
    "\\begin{align}\n",
    "z = \\frac{x-\\mu}{\\sigma}\n",
    "\\end{align}\n",
    "with x the feature value, $\\mu$ the mean and $\\sigma$ the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "g8HC-TDgB1D1"
   },
   "outputs": [],
   "source": [
    "# Calculate the Z-scores of each column in the training set:\n",
    "train_mean = train_mydata.mean()\n",
    "print('Mean of the training data:\\n',train_mean)\n",
    "train_std = train_mydata.std()\n",
    "print('Standard deviation of the training data:\\n',train_std)\n",
    "train_mydata_norm = train_mydata.copy()\n",
    "train_mydata_norm.number = (train_mydata.number - train_mean.number)/train_std.number\n",
    "train_mydata_norm.atomic_weight = (train_mydata.atomic_weight - train_mean.atomic_weight)/train_std.atomic_weight\n",
    "train_mydata_norm.covalent_radius = (train_mydata.covalent_radius - train_mean.covalent_radius)/train_std.covalent_radius\n",
    "train_mydata_norm.van_der_waals_radius = (train_mydata.van_der_waals_radius - train_mean.van_der_waals_radius)/train_std.van_der_waals_radius\n",
    "print(train_mydata_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "g8HC-TDgB1D1"
   },
   "outputs": [],
   "source": [
    "#calculate the Z-scores of each column in the test set:\n",
    "#be careful to apply the same transformation with mean / std from the training data!\n",
    "test_mydata_norm = test_mydata\n",
    "test_mydata_norm.number = (test_mydata.number - train_mean.number)/train_std.number\n",
    "test_mydata_norm.atomic_weight = (test_mydata.atomic_weight - train_mean.atomic_weight)/train_std.atomic_weight\n",
    "test_mydata_norm.covalent_radius = (test_mydata.covalent_radius - train_mean.covalent_radius)/train_std.covalent_radius\n",
    "test_mydata_norm.van_der_waals_radius = (test_mydata.van_der_waals_radius - train_mean.van_der_waals_radius)/train_std.van_der_waals_radius\n",
    "print(test_mydata_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9ehCgIRjTxy"
   },
   "source": [
    "## Represent data\n",
    "\n",
    "The following code cell creates a feature layer A containing three features:\n",
    "\n",
    "* the atomic weight;\n",
    "* the van der Waals radius;\n",
    "* the number of protons and electrons (the atomic number).\n",
    "\n",
    "It also generates a second feature layer B containing\n",
    "* the atomic weight;\n",
    "* the van der Waals radius;\n",
    "* the atomic weight x the number of protons and electrons (a feature cross).\n",
    "\n",
    "These are the features that the model will be trained on and it defines how each of those features will be represented. The transformations (collected in `my_feature_layer`) don't actually get applied until you pass a DataFrame to it, which will happen when we train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8EkNAQhnjSu-"
   },
   "outputs": [],
   "source": [
    "# Create an empty list that will eventually hold all created feature columns.\n",
    "feature_columns_A = []\n",
    "\n",
    "# We scaled all the columns, including latitude and longitude, into their\n",
    "# Z scores. So, instead of picking a resolution in degrees, we're going\n",
    "# to use resolution_in_Zs.  A resolution_in_Zs of 1 corresponds to \n",
    "# a full standard deviation. \n",
    "resolution_in_Zs = 0.3  # 3/10 of a standard deviation.\n",
    "\n",
    "# Represent atomic weight as floating-point value\n",
    "weight = tf.feature_column.numeric_column(\"atomic_weight\")\n",
    "feature_columns_A.append(weight)\n",
    "\n",
    "# Represent van der Waals radius as floating-point value\n",
    "vdW = tf.feature_column.numeric_column(\"van_der_waals_radius\")\n",
    "feature_columns_A.append(vdW)\n",
    "\n",
    "# Represent atomic number as floating-point value\n",
    "no_protons = tf.feature_column.numeric_column(\"number\")\n",
    "feature_columns_A.append(no_protons)\n",
    "\n",
    "# Create an empty list that will eventually hold all created feature columns\n",
    "feature_columns_B = []\n",
    "feature_columns_B.append(weight)\n",
    "\n",
    "# Create a bucket feature column for van der Waals radius\n",
    "vdW_boundaries = list(np.arange(int(min(train_mydata_norm['van_der_waals_radius'])), \n",
    "                                     int(max(train_mydata_norm['van_der_waals_radius'])), \n",
    "                                     resolution_in_Zs))\n",
    "vdW_b = tf.feature_column.bucketized_column(vdW, vdW_boundaries)\n",
    "feature_columns_B.append(vdW_b)\n",
    "\n",
    "# Create a bucket feature column for atomic weight\n",
    "weight_boundaries = list(np.arange(int(min(train_mydata_norm['atomic_weight'])), \n",
    "                                     int(max(train_mydata_norm['atomic_weight'])), \n",
    "                                     resolution_in_Zs))\n",
    "weight_b = tf.feature_column.bucketized_column(weight, weight_boundaries)\n",
    "\n",
    "# Create a bucket feature column for atomic number\n",
    "number_boundaries = list(np.arange(int(min(train_mydata_norm['number'])), \n",
    "                                     int(max(train_mydata_norm['number'])), \n",
    "                                     resolution_in_Zs))\n",
    "no_protons_b = tf.feature_column.bucketized_column(no_protons, number_boundaries)\n",
    "\n",
    "# Create a feature cross of atomic weight and atomic number\n",
    "# we use very litte binning thus the bucket size is 5\n",
    "# (this will combine the five closest values into one category)\n",
    "#vdW_x_number = tf.feature_column.crossed_column([vdW_b, no_protons_b], hash_bucket_size=5)\n",
    "weight_x_number = tf.feature_column.crossed_column([weight_b, no_protons_b], hash_bucket_size=5)\n",
    "#crossed_feature = tf.feature_column.indicator_column(vdW_x_number)\n",
    "crossed_feature = tf.feature_column.indicator_column(weight_x_number)\n",
    "feature_columns_B.append(crossed_feature)  \n",
    "\n",
    "# Convert the list of feature columns into a layer that will later be fed into\n",
    "# the model. \n",
    "my_feature_layer_A = tf.keras.layers.DenseFeatures(feature_columns_A)\n",
    "my_feature_layer_B = tf.keras.layers.DenseFeatures(feature_columns_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "QF0BFRXTOeR3"
   },
   "outputs": [],
   "source": [
    "#define the plotting function.\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "    plt.show()  \n",
    "\n",
    "print(\"Defined the plot_the_loss_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3014ezH3C7jT"
   },
   "source": [
    "## Define a deep neural net model\n",
    "\n",
    "The `create_model` function defines the topography of the deep neural net, specifying the following:\n",
    "\n",
    "* The number of [layers](https://developers.google.com/machine-learning/glossary/#layer) in the deep neural net.\n",
    "* The number of [nodes](https://developers.google.com/machine-learning/glossary/#node) in each layer.\n",
    "\n",
    "The `create_model` function also defines the [activation function](https://developers.google.com/machine-learning/glossary/#activation_function) of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "pedD5GhlDC-y"
   },
   "outputs": [],
   "source": [
    "def create_model(my_learning_rate, my_feature_layer):\n",
    "    \"\"\"Create and compile a regression model.\"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Add the layer containing the feature columns to the model.\n",
    "    model.add(my_feature_layer)\n",
    "\n",
    "    # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
    "    # method once for each layer. We've specified the following arguments:\n",
    "    #   * units specifies the number of nodes in this layer.\n",
    "    #   * activation specifies the activation function (Rectified Linear Unit).\n",
    "    #   * name is just a string that can be useful when debugging.\n",
    "\n",
    "    # Define the first hidden layer with 10 nodes.   \n",
    "    model.add(tf.keras.layers.Dense(units=10, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "    # Define the second hidden layer with 12 nodes. \n",
    "    model.add(tf.keras.layers.Dense(units=12, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden2'))\n",
    "\n",
    "    # Define the output layer.\n",
    "    model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                    name='Output'))                              \n",
    "  \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anH4A_yCcZx2"
   },
   "source": [
    "## Define a training function\n",
    "\n",
    "The `train_model` function trains the model from the input features and labels. The [tf.keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) method performs the actual training. The `x` parameter of the `fit` method is very flexible, enabling you to pass feature data in a variety of ways. The following implementation passes a Python dictionary in which:\n",
    "\n",
    "* The *keys* are the names of each feature (for example, `atomic_weight`, `number`, and so on).\n",
    "* The *value* of each key is a NumPy array containing the values of that feature. \n",
    "\n",
    "**Note:** Although you are passing *every* feature to `model.fit`, most of those values will be ignored. Only the features accessed by `my_feature_layer` will actually be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jv_lJYTcrEF"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None):\n",
    "    \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "    # Split the dataset into features and label.\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label = np.array(features.pop(label_name))\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True) \n",
    "\n",
    "    # The list of epochs is stored separately from the rest of history.\n",
    "    epochs = history.epoch\n",
    "  \n",
    "    # To track the progression of training, gather a snapshot\n",
    "    # of the model's mean squared error at each epoch. \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "    return epochs, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-IXYVfvM4gD"
   },
   "source": [
    "## Call the functions to build and train a deep neural net\n",
    "\n",
    "Okay, it is time to actually train the deep neural net.  If time permits, experiment with the three hyperparameters to see if you can reduce the loss\n",
    "against the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "nj3v5EKQFY8s"
   },
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 200\n",
    "batch_size = 80\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"covalent_radius\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, my_feature_layer_A)\n",
    "\n",
    "# Train the model on the normalized training set. We're passing the entire\n",
    "# normalized training set, but the model will only use the features\n",
    "# defined by the feature_layer.\n",
    "epochs, mse = train_model(my_model, train_mydata_norm, epochs, \n",
    "                          label_name, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.array(value) for name, value in test_mydata_norm.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mydata = test_mydata_norm.sample(frac=0.8,random_state=20)\n",
    "example_mydata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the evaluation\n",
    "example_features = {name:np.array(value) for name, value in example_mydata.items()}\n",
    "predicted = my_model.predict(example_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#un-normalize the predictions\n",
    "predicted = predicted * train_std.covalent_radius + train_mean.covalent_radius\n",
    "exact = example_mydata.covalent_radius * train_std.covalent_radius + train_mean.covalent_radius\n",
    "print('predicted values are:')\n",
    "print(predicted)\n",
    "print('the exact values are:')\n",
    "print(exact)\n",
    "print('deviation is:')\n",
    "for i in range(len(predicted)):\n",
    "    print(np.asarray([exact.values[i]])-predicted[i],'for atom:',example_mydata.symbol.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try this with the feature cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "nj3v5EKQFY8s"
   },
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 200\n",
    "batch_size = 80\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"covalent_radius\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, my_feature_layer_B)\n",
    "\n",
    "# Train the model on the normalized training set. We're passing the entire\n",
    "# normalized training set, but the model will only use the features\n",
    "# defined by the feature_layer.\n",
    "epochs, mse = train_model(my_model, train_mydata_norm, epochs, \n",
    "                          label_name, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.array(value) for name, value in test_mydata_norm.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mydata = test_mydata_norm.sample(frac=0.8,random_state=20)\n",
    "example_mydata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the evaluation\n",
    "example_features = {name:np.array(value) for name, value in example_mydata.items()}\n",
    "predicted = my_model.predict(example_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#un-normalize the predictions\n",
    "predicted = predicted * train_std.covalent_radius + train_mean.covalent_radius\n",
    "exact = example_mydata.covalent_radius * train_std.covalent_radius + train_mean.covalent_radius\n",
    "print('predicted values are:')\n",
    "print(predicted)\n",
    "print('the exact values are:')\n",
    "print(exact)\n",
    "print('deviation is:')\n",
    "for i in range(len(predicted)):\n",
    "    print(np.asarray([exact.values[i]])-predicted[i],'for atom:',example_mydata.symbol.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlPXK-SmmjQ2"
   },
   "source": [
    "# Task 1 #\n",
    "\n",
    "Compare the two models and change the hyperparameters / the model topography. Does the feature cross help here? Can you think of an example where a feature cross might be useful? What do you achieve using a feature cross?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pu7R_ZpDopIj"
   },
   "source": [
    "# Regularization\n",
    "\n",
    "Notice that in the below example, the model's loss against the test set is *much higher* than the loss against the training set.  In other words, the deep neural network is __overfitting__ to the data in the training set. To reduce overfitting, the model can be regularized, using:\n",
    "\n",
    "  * [L1 regularization](https://developers.google.com/machine-learning/glossary/#L1_regularization)\n",
    "  * [L2 regularization](https://developers.google.com/machine-learning/glossary/#L2_regularization)\n",
    "  * [Dropout regularization](https://developers.google.com/machine-learning/glossary/#dropout_regularization)\n",
    "\n",
    "Your task is to experiment with one or more regularization mechanisms to bring the test loss closer to the training loss (while still keeping test loss relatively low).  \n",
    "\n",
    "**Note:** When you add a regularization function to a model, you might need to tweak other hyperparameters. \n",
    "\n",
    "### Implementing L1 or L2 regularization\n",
    "\n",
    "To use L1 or L2 regularization on a hidden layer, specify the `kernel_regularizer` argument to [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense). Assign one of the following methods to this argument:\n",
    "\n",
    "* `tf.keras.regularizers.l1` for L1 regularization\n",
    "* `tf.keras.regularizers.l2` for L2 regularization\n",
    "\n",
    "Each of the preceding methods takes an `l` parameter, which adjusts the [regularization rate](https://developers.google.com/machine-learning/glossary/#regularization_rate). Assign a decimal value between 0 and 1.0 to `l`; the higher the decimal, the greater the regularization. For example, the following applies L2 regularization at a strength of 0.05. \n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dense(units=20, \n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(l=0.01),\n",
    "                                name='Hidden1'))\n",
    "```\n",
    "\n",
    "### Implementing Dropout regularization\n",
    "\n",
    "You implement dropout regularization as a separate layer in the topography. For example, the following code demonstrates how to add a dropout regularization layer between the first hidden layer and the second hidden layer:\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dense( *define first hidden layer*)\n",
    " \n",
    "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Dense( *define second hidden layer*)\n",
    "```\n",
    "\n",
    "The `rate` parameter to [tf.keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) specifies the fraction of nodes that the model should drop out during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(my_learning_rate, my_feature_layer):\n",
    "    \"\"\"Create and compile a regression model.\"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Add the layer containing the feature columns to the model.\n",
    "    model.add(my_feature_layer)\n",
    "\n",
    "    # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
    "    # method once for each layer. We've specified the following arguments:\n",
    "    #   * units specifies the number of nodes in this layer.\n",
    "    #   * activation specifies the activation function (Rectified Linear Unit).\n",
    "    #   * name is just a string that can be useful when debugging.\n",
    "\n",
    "    # Define the first hidden layer with 10 nodes.   \n",
    "    model.add(tf.keras.layers.Dense(units=30, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "    # Define the second hidden layer with 32 nodes. \n",
    "    model.add(tf.keras.layers.Dense(units=62, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden2'))\n",
    "    \n",
    "    # Define the third hidden layer with 32 nodes. \n",
    "    model.add(tf.keras.layers.Dense(units=80, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden3'))\n",
    "    \n",
    "    # Define the fourth hidden layer with 12 nodes. \n",
    "    model.add(tf.keras.layers.Dense(units=22, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden4'))\n",
    "\n",
    "    # Define the output layer.\n",
    "    model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                    name='Output'))                              \n",
    "  \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 800\n",
    "batch_size = 80\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"covalent_radius\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model2(learning_rate, my_feature_layer_A)\n",
    "\n",
    "# Train the model on the normalized training set. We're passing the entire\n",
    "# normalized training set, but the model will only use the features\n",
    "# defined by the feature_layer.\n",
    "epochs, mse = train_model(my_model, train_mydata_norm, epochs, \n",
    "                          label_name, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.array(value) for name, value in test_mydata_norm.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "tflt9TZEDARW"
   },
   "outputs": [],
   "source": [
    "# in this example, L2 regularization is used\n",
    "def create_model_with_penalty(my_learning_rate, my_feature_layer):\n",
    "    \"\"\"Create and compile a regression model.\"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Add the layer containing the feature columns to the model.\n",
    "    model.add(my_feature_layer)\n",
    "\n",
    "    # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
    "    # method once for each layer. We've specified the following arguments:\n",
    "    #   * units specifies the number of nodes in this layer.\n",
    "    #   * activation specifies the activation function (Rectified Linear Unit).\n",
    "    #   * name is just a string that can be useful when debugging.\n",
    "\n",
    "    # Define the first hidden layer with 10 nodes.   \n",
    "    model.add(tf.keras.layers.Dense(units=30, \n",
    "                                  activation='relu', \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "    # Define the second hidden layer with 32 nodes. \n",
    "    model.add(tf.keras.layers.Dense(units=62, \n",
    "                                  activation='relu', \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
    "                                  name='Hidden2'))\n",
    "    \n",
    "    # Define the third hidden layer with 32 nodes. \n",
    "    model.add(tf.keras.layers.Dense(units=80, \n",
    "                                  activation='relu', \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
    "                                  name='Hidden3'))\n",
    "    \n",
    "    # Define the fourth hidden layer with 12 nodes. \n",
    "    model.add(tf.keras.layers.Dense(units=22, \n",
    "                                  activation='relu', \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
    "                                  name='Hidden4'))\n",
    "\n",
    "    # Define the output layer.\n",
    "    model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                    name='Output'))                              \n",
    "  \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 800\n",
    "batch_size = 80\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"covalent_radius\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model_with_penalty(learning_rate, my_feature_layer_A)\n",
    "\n",
    "# Train the model on the normalized training set. We're passing the entire\n",
    "# normalized training set, but the model will only use the features\n",
    "# defined by the feature_layer.\n",
    "epochs, mse = train_model(my_model, train_mydata_norm, epochs, \n",
    "                          label_name, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.array(value) for name, value in test_mydata_norm.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Try out different regularization mechanisms. Why is overfitting leading to better performance of the model on the data of the training vs the test set? What does regularization enforce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict flight delays in the US in January: Binary classification\n",
    "\n",
    "The dataset is taken from here: \n",
    "\n",
    "https://www.kaggle.com/divyansh22/flight-delay-prediction\n",
    "\n",
    "We are interested in the columns: OP_CARRIER_AIRLINE_ID (which airline), ORIGIN (originating airport), DEST (destination airport), DEP_TIME (departure time), DEP_DEL15 (a '1' indicates if the departure was delayed 15 minutes or more, 0 is on time), ARR_TIME (arrival time), ARR_DEL15 (a '1' indicates if the departure was delayed 15 minutes or more, 0 is on time), CANCELLED (flight was cancelled), DIVERTED (flight was diverted), DISTANCE (distance traveled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('Jan_2019_ontime.csv',encoding = 'cp1252',engine='python')\n",
    "test_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([test_df['OP_CARRIER_AIRLINE_ID'], test_df['ORIGIN'], test_df['DEST'], \n",
    "                    test_df['DEP_TIME'], test_df['DEP_DEL15'], test_df['ARR_TIME'], test_df['ARR_DEL15'],\n",
    "                    test_df['CANCELLED'],test_df['DIVERTED'], test_df['DISTANCE']], axis=1)\n",
    "# fill the empty cells with nan\n",
    "df.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all nan's\n",
    "my_df = df.dropna()\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Departure delay and arrival delay are related. Departure time and arrival time are related. There is a slight correlation between arrival delay and departure time, and departure delay and departure time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_mydata, test_mydata = train_test_split(my_df, test_size=0.2)\n",
    "train_mydata, val_mydata = train_test_split(train_mydata, test_size=0.2)\n",
    "print(len(train_mydata), 'train examples')\n",
    "print(len(val_mydata), 'validation examples')\n",
    "print(len(test_mydata), 'test examples')\n",
    "#train_mydata = my_df.sample(frac=0.8,random_state=20)\n",
    "#test_mydata = my_df.drop(train_mydata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "g8HC-TDgB1D1"
   },
   "outputs": [],
   "source": [
    "# Calculate the Z-scores of selected columns in the training set: this only makes \n",
    "# sense for distance\n",
    "train_mean_distance = train_mydata.DISTANCE.mean()\n",
    "train_std_distance = train_mydata.DISTANCE.std()\n",
    "train_df_norm = train_mydata.copy()\n",
    "#Rescale the times so they lie between 0 and 1 and not 0 and 2400\n",
    "#this could be done in a better way as the time is given as hhmm.0\n",
    "#and we could account for mm having values between 0 and 59 but at this \n",
    "#point we do not want to complicate things\n",
    "train_df_norm.ARR_TIME = train_mydata.ARR_TIME/2400\n",
    "train_df_norm.DEP_TIME = train_mydata.DEP_TIME/2400\n",
    "train_df_norm.DISTANCE = (train_mydata.DISTANCE - train_mean_distance)/train_std_distance\n",
    "train_df_norm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_norm = test_mydata.copy()\n",
    "test_df_norm.ARR_TIME = test_mydata.ARR_TIME/2400\n",
    "test_df_norm.DEP_TIME = test_mydata.DEP_TIME/2400\n",
    "test_df_norm.DISTANCE = (test_mydata.DISTANCE - train_mean_distance)/train_std_distance\n",
    "test_df_norm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_norm = val_mydata.copy()\n",
    "val_df_norm.ARR_TIME = val_mydata.ARR_TIME/2400\n",
    "val_df_norm.DEP_TIME = val_mydata.DEP_TIME/2400\n",
    "val_df_norm.DISTANCE = (val_mydata.DISTANCE - train_mean_distance)/train_std_distance\n",
    "val_df_norm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of all airports in origin and destination\n",
    "df_cat = my_df.ORIGIN.astype('category')\n",
    "airport_list = df_cat.cat.categories.tolist()\n",
    "df_cat = my_df.DEST.astype('category')\n",
    "airport_list_d = df_cat.cat.categories.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(airport_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(airport_list)\n",
    "temp = [item for item in airport_list if item not in airport_list_d]\n",
    "print('Difference in origin and destination airports:',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all the airline id's\n",
    "al_cat = my_df.OP_CARRIER_AIRLINE_ID.astype('category')\n",
    "al_list = al_cat.cat.categories.tolist()\n",
    "print(al_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8EkNAQhnjSu-"
   },
   "outputs": [],
   "source": [
    "# Create an empty list that will eventually hold all created feature columns.\n",
    "feature_columns = []\n",
    "\n",
    "# We scaled all the columns, including latitude and longitude, into their\n",
    "# Z scores. So, instead of picking a resolution in degrees, we're going\n",
    "# to use resolution_in_Zs.  A resolution_in_Zs of 1 corresponds to \n",
    "# a full standard deviation. \n",
    "resolution_in_Zs = 0.1  # 1/10 of a standard deviation.\n",
    "\n",
    "# Create a bucket feature column for departure time\n",
    "dep = tf.feature_column.numeric_column('DEP_TIME')\n",
    "dep_boundaries = list(np.arange(int(min(train_df_norm['DEP_TIME'])), \n",
    "                                     int(max(train_df_norm['DEP_TIME'])), \n",
    "                                     resolution_in_Zs))\n",
    "#print(min(train_df_norm['DEP_TIME']),max(train_df_norm['DEP_TIME']),dep_boundaries)\n",
    "dep_b = tf.feature_column.bucketized_column(dep, dep_boundaries)\n",
    "feature_columns.append(dep_b)\n",
    "\n",
    "# Create a bucket feature column for arrival time\n",
    "arr = tf.feature_column.numeric_column('ARR_TIME')\n",
    "arr_boundaries = list(np.arange(int(min(train_df_norm['ARR_TIME'])), \n",
    "                                     int(max(train_df_norm['ARR_TIME'])), \n",
    "                                     resolution_in_Zs))\n",
    "arr_b = tf.feature_column.bucketized_column(arr, arr_boundaries)\n",
    "feature_columns.append(arr_b)\n",
    "\n",
    "# Create a bucket feature column for distance\n",
    "dist = tf.feature_column.numeric_column('DISTANCE')\n",
    "dist_boundaries = list(np.arange(int(min(train_df_norm['DISTANCE'])), \n",
    "                                     int(max(train_df_norm['DISTANCE'])), \n",
    "                                     resolution_in_Zs))\n",
    "dist_b = tf.feature_column.bucketized_column(dist, dist_boundaries)\n",
    "feature_columns.append(dist_b)\n",
    "\n",
    "# Create a categorical feature column for originating airport\n",
    "origin_c = tf.feature_column.categorical_column_with_vocabulary_list('ORIGIN',airport_list)\n",
    "# Create an embedded feature column for originating airport\n",
    "# the dimension has to be fine-tuned\n",
    "origin_e = tf.feature_column.embedding_column(origin_c,dimension=1)\n",
    "feature_columns.append(origin_e)\n",
    "\n",
    "# Create a categorical feature column for destination airport\n",
    "dest_c = tf.feature_column.categorical_column_with_vocabulary_list('DEST',airport_list_d)\n",
    "# Create an embedded feature column for destination airport\n",
    "# the dimension has to be fine-tuned\n",
    "dest_e = tf.feature_column.embedding_column(dest_c,dimension=1)\n",
    "feature_columns.append(dest_e)\n",
    "\n",
    "# Create a one-hot categorical feature column for airline id\n",
    "airline_c = tf.feature_column.categorical_column_with_vocabulary_list('OP_CARRIER_AIRLINE_ID',al_list)\n",
    "# Create an embedded feature column for airline id\n",
    "airline_e = tf.feature_column.embedding_column(airline_c,dimension=1)\n",
    "feature_columns.append(airline_e)\n",
    "\n",
    "#we want to predict arrival delay and departure delay\n",
    "# Convert the list of feature columns into a layer that will later be fed into\n",
    "# the model. \n",
    "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "QF0BFRXTOeR3"
   },
   "outputs": [],
   "source": [
    "#@title Define the plotting function.\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    # list_of_metrics should be one of the names shown in:\n",
    "    # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "print(\"Defined the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3014ezH3C7jT"
   },
   "source": [
    "## Define a neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "pedD5GhlDC-y"
   },
   "outputs": [],
   "source": [
    "#@title Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, feature_layer, my_metrics):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the feature layer (the list of features and how they are represented)\n",
    "    # to the model.\n",
    "    model.add(feature_layer)\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid),)\n",
    "  \n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.  Notice that we're using a different loss\n",
    "    # function for classification than for regression.    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),                                                   \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anH4A_yCcZx2"
   },
   "source": [
    "## Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jv_lJYTcrEF"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "    # The x parameter of tf.keras.Model.fit can be a list of arrays, where\n",
    "    # each array contains the data for one feature.  Here, we're passing\n",
    "    # every column in the dataset. Note that the feature_layer will filter\n",
    "    # away most of those columns, leaving only the desired columns and their\n",
    "    # representations as features.\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label = np.array(features.pop(label_name)) \n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "    # The list of epochs is stored separately from the rest of history.\n",
    "    epochs = history.epoch\n",
    "\n",
    "    # Isolate the classification metric for each epoch.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-IXYVfvM4gD"
   },
   "source": [
    "## Call the functions to build and train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_norm.ARR_DEL15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "nj3v5EKQFY8s"
   },
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"ARR_DEL15\"\n",
    "\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.4\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "#metric = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),]\n",
    "metric = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(thresholds=classification_threshold,name='precision'),\n",
    "      tf.keras.metrics.Recall(thresholds=classification_threshold,name='recall'),]\n",
    "          \n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, my_feature_layer,metric)\n",
    "\n",
    "# Train the model on the training set.\n",
    "# Train the model on the normalized training set. We're passing the entire\n",
    "# normalized training set, but the model will only use the features\n",
    "# defined by the feature_layer.\n",
    "epochs, hist = train_model(my_model, train_df_norm, epochs, \n",
    "                           label_name, batch_size)\n",
    "\n",
    "# Plot a graph of the metric(s) vs. epochs.\n",
    "#list_of_metrics_to_plot = ['accuracy'] \n",
    "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall'] \n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a prediction\n",
    "example_features = {name:np.array(value) for name, value in val_df_norm.items()}\n",
    "predicted = my_model.predict(example_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact = val_df_norm.ARR_DEL15\n",
    "for i in range(100):\n",
    "    print(exact.values[i],(predicted[i] > .4).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional task\n",
    "\n",
    "It also works if you do not complete the optional task. Every bit that you try will aid your learning. Highly recommended, but as I cannot force you to create a kaggle account, this is still optional.\n",
    "\n",
    "1. Find a dataset on kaggle that looks interesting to you and that has reasonable data (not too small/too large, not too much missing/irregular data, formatting is ok).\n",
    "\n",
    "2. Represent this dataset using pandas. Find correlations in the data.\n",
    "\n",
    "3. Create a simple model to make predictions about the data.\n",
    "\n",
    "Upload your notebook to moodle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Intro to Neural Nets.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
